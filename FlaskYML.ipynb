{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZLnP-OF0Mcm",
        "outputId": "76faf3df-fbff-4b57-ae5d-cbbcea5be740"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Using cached openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAQxAi4X0P9y",
        "outputId": "f030680e-381c-42b1-9987-8739bb3b4274"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import pandas as pd\n",
        "import os\n",
        "import openai\n",
        "import requests\n",
        "import re\n",
        "import time\n",
        "import io\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "cLsRfDIJ0K26"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "YpCbPLPZzzK9"
      },
      "outputs": [],
      "source": [
        "def generate_nlu_files_from_pdf(pdf_content, api_key, que):\n",
        "\n",
        "\n",
        "    openai.api_key = api_key\n",
        "\n",
        "    # Wrap the entire function with tqdm context manager\n",
        "    with tqdm(total=144, desc=\"Generating YML Files\", unit='B', unit_scale=True) as pbar:\n",
        "      def extract_text_from_pdf(pdf_content):\n",
        "          text = \"\"\n",
        "          reader = PyPDF2.PdfReader(pdf_content)\n",
        "          pbar.update(1)\n",
        "          for page in reader.pages:\n",
        "              text += page.extract_text()\n",
        "              pbar.update(1)\n",
        "          return text\n",
        "          pbar.update(1)\n",
        "      pbar.update(1)\n",
        "\n",
        "      extracted_text = extract_text_from_pdf(pdf_content)\n",
        "      pbar.update(1)\n",
        "      def get_completion(text_content):\n",
        "\n",
        "          user_message = f\"\"\"\n",
        "          Give me {que} questions that can be answered with the information in text_content.\n",
        "          What are the corresponding answers?\n",
        "          \"\"\"\n",
        "\n",
        "          messages = [\n",
        "              {'role':'system', 'content': text_content},\n",
        "              {'role':'user', 'content': f'####{user_message}####'}\n",
        "          ]\n",
        "          pbar.update(1)\n",
        "          response = openai.ChatCompletion.create(\n",
        "              model=\"gpt-3.5-turbo\",\n",
        "              messages=messages,\n",
        "              temperature=0,\n",
        "              max_tokens=500\n",
        "          )\n",
        "\n",
        "          return response.choices[0].message[\"content\"]\n",
        "      pbar.update(1)\n",
        "      qa_pairs = get_completion(extracted_text)\n",
        "      sentences = qa_pairs.split('\\n')\n",
        "      pbar.update(1)\n",
        "      # List of default GPT-3 responses to filter out\n",
        "      FILTER_RESPONSES = [\n",
        "      \"I can provide five questions and their corresponding answers based on the above text:\",\n",
        "      f\"I'm sorry, as an AI language model, I cannot provide a list of {que} questions and their corresponding answers without any context. Could you please provide more information or specify the questions you would like me to answer?\"\n",
        "      ]\n",
        "\n",
        "      # Filter sentences\n",
        "      sentences = [s for s in sentences if s not in FILTER_RESPONSES and s.strip()]\n",
        "      pbar.update(1)\n",
        "\n",
        "      df=pd.DataFrame(columns=['question','answer'])\n",
        "      pbar.update(1)\n",
        "      for i in range(0, len(sentences), 2):\n",
        "          if i + 1 < len(sentences):\n",
        "              df.loc[len(df)] = {\n",
        "                  'question': sentences[i],\n",
        "                  'answer': sentences[i + 1],\n",
        "              }\n",
        "              pbar.update(1)\n",
        "          else:\n",
        "              df.loc[len(df)] = {\n",
        "                  'question': sentences[i],\n",
        "                  'answer': '',\n",
        "              }\n",
        "              pbar.update(1)\n",
        "\n",
        "\n",
        "      df['answer']=df['answer'].apply(lambda x: re.sub(r'Answer: ','',x))\n",
        "      df['question']=df['question'].apply(lambda x: re.sub(r'\\d+\\.\\s','',x))\n",
        "      df['answer']=df['answer'].apply(lambda x: re.sub(r'^\\d+\\.\\s*','',x))\n",
        "      df['question']=df['question'].apply(lambda x: re.sub(r'Q:','',x))\n",
        "      df['answer']=df['answer'].apply(lambda x: re.sub(r'Answer:','',x))\n",
        "      pbar.update(1)\n",
        "\n",
        "      # Identify duplicate values in the 'question' column\n",
        "      duplicate_mask = df.duplicated(subset=['question'], keep='first')\n",
        "      pbar.update(1)\n",
        "\n",
        "      # Keep only the unique values (first occurrence of each question)\n",
        "      df_unique = df[~duplicate_mask]\n",
        "      pbar.update(1)\n",
        "\n",
        "      # Function to get the response from ChatGPT through API key\n",
        "      def get_completion_from_messages(messages,\n",
        "                                      model=\"gpt-3.5-turbo\",\n",
        "                                      temperature=0,\n",
        "                                      max_tokens=1000):\n",
        "          response = openai.ChatCompletion.create(\n",
        "              model=model,\n",
        "              messages=messages,\n",
        "              temperature=temperature,\n",
        "              max_tokens=max_tokens,\n",
        "          )\n",
        "          pbar.update(1)\n",
        "          return response.choices[0].message[\"content\"]\n",
        "      pbar.update(1)\n",
        "      # Function to extract entities from question\n",
        "      def ent_extract(i):\n",
        "          \"\"\"\n",
        "          This function takes question statement from the dataframe of question and answers and return entities\n",
        "          \"\"\"\n",
        "          messages =  [\n",
        "              {'role':'system',\n",
        "              'content':\"\"\"You are a helpful assistant who performs entity extraction. Your answer should be the primary and  one secondary entity , with nothing else.\n",
        "              Primary entity should be only program title - or facaulty name like \"\"\"},\n",
        "              {'role':'user',\n",
        "              'content':\"\"\"Extract primary and secondary entities from this question statement - {}\"\"\".format(i)},\n",
        "          ]\n",
        "          pbar.update(1)\n",
        "          response = get_completion_from_messages(messages)\n",
        "          pbar.update(1)\n",
        "          return response\n",
        "\n",
        "\n",
        "      entities = [ent_extract(i) for i in df_unique[:]['question']]\n",
        "      pbar.update(1)\n",
        "\n",
        "      df_unique['entities'] = entities\n",
        "      pbar.update(1)\n",
        "\n",
        "      primary_entities=[]\n",
        "      secondary_entities=[]\n",
        "      pbar.update(1)\n",
        "      for i in range(len(df_unique)):\n",
        "          samples=df_unique['entities'][i].split('Secondary')\n",
        "          primary_entities.append(samples[0])\n",
        "          if len(samples)>1:\n",
        "              secondary_entities.append(samples[1])\n",
        "          else:\n",
        "              secondary_entities.append('')\n",
        "\n",
        "      pbar.update(1)\n",
        "      def preprocessin(entities):\n",
        "          entities=list(map(lambda x: re.sub('(Primary|Secondary)','',x).strip(),  entities))\n",
        "          entities=list(map(lambda x: re.sub('Entity','',x),  entities))\n",
        "          entities=list(map(lambda x: re.sub(r'\\s+at Yeshiva University','',x),  entities))\n",
        "          pbar.update(1)\n",
        "          entities=list(map(lambda x: re.sub(r'\\s+at the katz school','',x),  entities))\n",
        "          entities=list(map(lambda x: re.sub(r'(\\s+program\\s*|\\s+in\\s+)',' ',x),  entities))\n",
        "          entities=list(map(lambda x: re.sub(r'(\\s+and\\s+)',' ',x),  entities))\n",
        "          entities=list(map(lambda x: re.sub('(entity|entities)','',x),  entities))\n",
        "          pbar.update(1)\n",
        "          entities=list(map(lambda x: re.sub('(:|\\')','',x),  entities))\n",
        "          entities = list(map(lambda x: re.sub(r'(\\\\n|\\\\n=)', '', x, flags=re.MULTILINE), entities))\n",
        "          entities = list(map(lambda x: re.sub(r'(masters degree|masters)', 'm.s.', x, flags=re.MULTILINE), entities))\n",
        "          entities = list(map(lambda x: re.sub(r'(/|\\.)', '', x, flags=re.MULTILINE), entities))\n",
        "          entities = list(map(lambda x: re.sub(r'(/|\\.)', '', x, flags=re.MULTILINE), entities))\n",
        "          pbar.update(1)\n",
        "          entities = list(map(lambda x: x.strip().lower(), entities))\n",
        "\n",
        "          return entities\n",
        "\n",
        "      pbar.update(1)\n",
        "      pbar.update(1)\n",
        "      primary_entities = preprocessin(primary_entities)\n",
        "      pbar.update(1)\n",
        "      secondary_entities = preprocessin(secondary_entities)\n",
        "\n",
        "      df_unique['primary_entities']=primary_entities\n",
        "      df_unique['secondary_entities']=secondary_entities\n",
        "      pbar.update(1)\n",
        "      del df_unique['entities']\n",
        "\n",
        "      df_unique['unique_intents']=df_unique['primary_entities']+' '+df_unique['secondary_entities']\n",
        "      pbar.update(1)\n",
        "      df_unique['unique_intents']=df_unique['unique_intents'].apply(lambda x: re.sub(' ','',x))\n",
        "\n",
        "      def get_completion(prompt, model=\"gpt-3.5-turbo\", retries=3):\n",
        "          for _ in range(retries):\n",
        "              try:\n",
        "                  messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "                  response = openai.ChatCompletion.create(\n",
        "                      model=model,\n",
        "                      messages=messages,\n",
        "                      temperature=0, # this is the degree of randomness of the model's output\n",
        "                  )\n",
        "                  pbar.update(1)\n",
        "                  return response.choices[0].message[\"content\"]\n",
        "              except openai.error.ServiceUnavailableError as e:\n",
        "                  print(f\"Service Unavailable. Retrying after 5 seconds... Error: {e}\")\n",
        "                  time.sleep(5)\n",
        "              except openai.error.APIError as e:\n",
        "                  print(f\"API Error. Retrying after 5 seconds... Error: {e}\")\n",
        "                  time.sleep(5)\n",
        "          raise Exception(\"API call failed after multiple retries.\")\n",
        "\n",
        "      pbar.update(1)\n",
        "      # Function for generating intents with examples for each question\n",
        "\n",
        "      def intents_examples(question):\n",
        "\n",
        "          text = question\n",
        "\n",
        "          prompt = f\"\"\"\n",
        "          Generate atleast ten similar short questions for the question given in the text\n",
        "\n",
        "          \\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
        "          \"\"\"\n",
        "          pbar.update(1)\n",
        "          response = get_completion(prompt)\n",
        "          return response\n",
        "\n",
        "      pbar.update(1)\n",
        "      # Add a new column 'examples' to store the generated examples\n",
        "      df_unique['examples'] = \"\"\n",
        "\n",
        "      # Iterate over each row in the DataFrame and generate examples for each question\n",
        "      for idx, row in df_unique.iterrows():\n",
        "          question = row['question']\n",
        "          examples = intents_examples(question)\n",
        "          df_unique.at[idx, 'examples'] = examples\n",
        "          pbar.update(1)\n",
        "\n",
        "\n",
        "      # Function to generate nlu.yml file for each row\n",
        "\n",
        "      def generate_nlu_section(df):\n",
        "          nlu_content = \"\"\n",
        "          for _, row in df.iterrows():\n",
        "              question = row['question']\n",
        "              unique_intent = row['unique_intents']\n",
        "              examples = row['examples'].split('\\n')\n",
        "\n",
        "              nlu_content += f\"- intent: {unique_intent}\\n\"\n",
        "              nlu_content += \"  examples: |\\n\"\n",
        "\n",
        "              nlu_content += f\"    - {question}\\n\"\n",
        "\n",
        "              for example in examples:\n",
        "                  if example.strip():\n",
        "                      # Remove numbering (e.g., \"1.\", \"2.\", etc.) from the examples\n",
        "                      example_text = \" \".join(example.split()[1:])\n",
        "                      nlu_content += f\"    - {example_text.strip()}\\n\"\n",
        "\n",
        "              nlu_content += \"\\n\"\n",
        "\n",
        "          return nlu_content\n",
        "          pbar.update(1)\n",
        "      pbar.update(1)\n",
        "      # Generate nlu.yml content\n",
        "      nlu_content = generate_nlu_section(df_unique)\n",
        "\n",
        "      # Save the nlu content to nlu.yml file\n",
        "      with open(\"nlu.yml\", \"w\") as file:\n",
        "          file.write(nlu_content)\n",
        "\n",
        "      pbar.update(1)\n",
        "      # Function to generate stories.yml file for each row\n",
        "\n",
        "      def generate_stories_yaml_row(unique_intents):\n",
        "          yml_content = (\n",
        "              f\"- story: {unique_intents}\\n\"\n",
        "              f\"  steps:\\n\"\n",
        "              f\"  - intent: {unique_intents}\\n\"\n",
        "              f\"  - action: utter_{unique_intents}\\n\"\n",
        "          )\n",
        "          return yml_content\n",
        "          pbar.update(1)\n",
        "\n",
        "      # Convert unique intents to YAML format\n",
        "      yml_content = \"\\n\".join(df_unique.apply(lambda row: generate_stories_yaml_row(row[\"unique_intents\"]), axis=1))\n",
        "\n",
        "      # Save the YAML content to a file\n",
        "      with open(\"stories.yml\", \"w\") as file:\n",
        "          file.write(yml_content)\n",
        "          pbar.update(1)\n",
        "\n",
        "\n",
        "      pbar.update(1)\n",
        "      # Functions to generate domain.yml file for each row\n",
        "\n",
        "      # Function to generate intents section in YML format\n",
        "      def generate_intents_section(df):\n",
        "          #intents = \"\\n  - \".join(df['unique_intents'])\n",
        "          intents = \"\\n- \".join(df['unique_intents'])\n",
        "          return f\"intents:\\n- {intents}\\n\\n\"\n",
        "          pbar.update(1)\n",
        "\n",
        "      # Function to generate actions section in YML format\n",
        "      def generate_actions_section(df):\n",
        "          actions = \"\\n- \".join([f\"utter_{intent}\" for intent in df['unique_intents']])\n",
        "          return f\"actions:\\n- {actions}\\n\\n\"\n",
        "          pbar.update(1)\n",
        "\n",
        "      # Function to generate responses section in YAML format\n",
        "      def generate_responses_section(df):\n",
        "          responses = \"responses:\\n\"\n",
        "          for _, row in df.iterrows():\n",
        "              responses += f\"  utter_{row['unique_intents']}:\\n  - text: {row['answer']}\\n\\n\"\n",
        "          return responses\n",
        "          pbar.update(1)\n",
        "\n",
        "      # Generate YML content\n",
        "      yml_content = generate_intents_section(df_unique)\n",
        "      pbar.update(1)\n",
        "      yml_content += generate_actions_section(df_unique)\n",
        "      pbar.update(1)\n",
        "      yml_content += generate_responses_section(df_unique)\n",
        "      pbar.update(1)\n",
        "\n",
        "      pbar.update(1)\n",
        "      # Save the YML content to a file\n",
        "      with open(\"domain.yml\", \"w\") as file:\n",
        "          file.write(yml_content)\n",
        "          pbar.update(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "pdf_path = '/content/MKT_Resume.pdf'  # Replace with the actual path\n",
        "api_key = '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' # Replace with your OpenAI API key\n",
        "que = 15\n",
        "generate_nlu_files_from_pdf(pdf_path, api_key, que)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5_bxpH50U84",
        "outputId": "06a57f1f-06ba-424a-8175-8dd8a9ee743e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating YML Files: 100%|██████████| 144/144 [01:18<00:00, 1.84B/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UGFhIOqK1-St"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}